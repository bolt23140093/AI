# coding: utf-8


import numpy as np
import os
import pandas as pd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap


# # Artificial neurons - a brief glimpse into the early history of machine learning





# ## The formal definition of an artificial neuron





# ## The perceptron learning rule



# # Implementing a perceptron learning algorithm in Python

# ## An object-oriented perceptron API





class Perceptron(object):#æ©Ÿå™¨å­¸ç¿’æ¨¡åž‹
    """Perceptron classifier.

    Parameters
    ------------
    eta : float
      Learning rate (between 0.0 and 1.0)
    n_iter : int
      Passes over the training dataset.
    random_state : int
      Random number generator seed for random weight
      initialization.

    Attributes
    -----------
    w_ : 1d-array
      Weights after fitting.
    errors_ : list
      Number of misclassifications (updates) in each epoch.

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y): #X=feature, Y=Label
        """Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_examples, n_features]
          Training vectors, where n_examples is the number of examples and
          n_features is the number of features.
        y : array-like, shape = [n_examples]
          Target values.

        Returns
        -------
        self : object

        """
        rgen = np.random.RandomState(self.random_state)#ç¢ºä¿æ¯æ¬¡å¯¦é©—ã€Œéš¨æ©Ÿåˆå§‹åŒ–ã€æ˜¯ä¸€æ¨£çš„
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])#åˆå§‹åŒ–æ¬Šé‡å‘é‡ï¼ˆå°å°çš„éš¨æ©Ÿæ•¸ï¼‰ï¼Œæœ‰ 1 + ç‰¹å¾µæ•¸é‡ å€‹å…ƒç´ ã€‚ï¼ˆå› ç‚ºæœ‰ä¸€å€‹æ˜¯ bias æˆªè·é …ï¼‰
        self.errors_ = []

        for _ in range(self.n_iter):
            errors = 0
            for xi, target in zip(X, y):
                update = self.eta * (target - self.predict(xi))
                self.w_[1:] += update * xi
                self.w_[0] += update
                errors += int(update != 0.0)
            self.errors_.append(errors)
            """å°æ¯ä¸€å€‹æ¨£æœ¬ (xi, target)ï¼š
                self.predict(xi)ï¼šç”¨ç›®å‰çš„æ¬Šé‡é æ¸¬ xi çš„æ¨™ç±¤ã€‚
                (target - self.predict(xi))ï¼šçœ‹é æ¸¬éŒ¯å¤šå°‘ã€‚
                update = å­¸ç¿’çŽ‡ * (ç›®æ¨™å€¼ - é æ¸¬å€¼)ï¼šè¨ˆç®—ã€Œèª¿æ•´é‡ã€ã€‚
                æ›´æ–°æ¬Šé‡ï¼šself.w_[1:] += update * xi
                æ›´æ–°åç½®ï¼ˆbiasï¼‰ï¼šself.w_[0] += update
                å¦‚æžœæœ‰æ›´æ–°ï¼ˆupdate â‰  0ï¼‰ï¼Œå°±è¨˜ä¸€æ¬¡éŒ¯èª¤ã€‚"""
        return self

    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def predict(self, X):
        """Return class label after unit step"""
        return np.where(self.net_input(X) >= 0.0, 1, -1)




v1 = np.array([1, 2, 3])
v2 = 0.5 * v1
np.arccos(v1.dot(v2) / (np.linalg.norm(v1) * np.linalg.norm(v2)))
"""
v1.dot(v2)ï¼šè¨ˆç®— v1 å’Œ v2 çš„å…§ç©ã€‚
np.linalg.norm(v1)ï¼šè¨ˆç®— v1 çš„é•·åº¦ï¼ˆL2 normï¼‰ã€‚
np.linalg.norm(v2)ï¼šè¨ˆç®— v2 çš„é•·åº¦ã€‚
æ‰€ä»¥ v1.dot(v2) / (||v1|| * ||v2||) æ˜¯ä»€éº¼ï¼ŸðŸ‘‰ å®ƒå°±æ˜¯cos(Î¸)ï¼Œä¹Ÿå°±æ˜¯å…©å‘é‡ä¹‹é–“å¤¾è§’çš„é¤˜å¼¦å€¼ã€‚
np.arccos(...)ï¼šå†ç”¨åé¤˜å¼¦ï¼ˆarccosï¼‰æŠŠ cos(Î¸) è½‰å›žçœŸæ­£çš„è§’åº¦ï¼ˆå¼§åº¦å–®ä½ï¼‰ã€‚
ç¸½çµä¾†èªªï¼šé€™ä¸€è¡Œå°±æ˜¯ç®—å‡ºã€Œv1 å’Œ v2 ä¹‹é–“çš„å¤¾è§’ Î¸ï¼ˆå¼§åº¦ï¼‰ã€ã€‚
"""


# ## Training a perceptron model on the Iris dataset

# ...

# ### Reading-in the Iris data





s = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'
print('URL:', s)

df = pd.read_csv(s,
                 header=None,
                 encoding='utf-8')

df.tail()


# 
# ### Note:
# 
# 
# You can find a copy of the Iris dataset (and all other datasets used in this book) in the code bundle of this book, which you can use if you are working offline or the UCI server at https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data is temporarily unavailable. For instance, to load the Iris dataset from a local directory, you can replace the line 
# 
#     df = pd.read_csv('https://archive.ics.uci.edu/ml/'
#         'machine-learning-databases/iris/iris.data', header=None)
#  
# by
#  
#     df = pd.read_csv('your/local/path/to/iris.data', header=None)
# 



df = pd.read_csv('iris.data', header=None, encoding='utf-8')
df.tail()




# ### Plotting the Iris data




# select setosa and versicolor
y = df.iloc[0:100, 4].values
y = np.where(y == 'Iris-setosa', -1, 1)
"""
np.where(æ¢ä»¶, å¦‚æžœæ˜¯Trueçš„çµæžœ, å¦‚æžœæ˜¯Falseçš„çµæžœ)
y == 'Iris-setosa'ï¼šæª¢æŸ¥æ¯å€‹ y è£¡çš„å…ƒç´ æ˜¯ä¸æ˜¯ 'Iris-setosa'
å¦‚æžœæ˜¯ 'Iris-setosa'ï¼Œå°±æŠŠå®ƒè®Šæˆ -1
å¦‚æžœä¸æ˜¯ 'Iris-setosa'ï¼Œå°±è®Šæˆ 1
"""

# extract sepal length and petal length
X = df.iloc[0:100, [0, 2]].values

# plot data
plt.scatter(X[:50, 0], X[:50, 1],
            color='red', marker='o', label='setosa')
plt.scatter(X[50:100, 0], X[50:100, 1],
            color='blue', marker='x', label='versicolor')

plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')

# plt.savefig('images/02_06.png', dpi=300)
plt.show()



# ### Training the perceptron model



ppn = Perceptron(eta=0.1, n_iter=10)
#å»ºç«‹ä¸€å€‹ Perceptron ç‰©ä»¶ï¼Œè¨­å®šå­¸ç¿’çŽ‡(æ¯æ¬¡èª¿æ•´çš„å¹…åº¦) Î· = 0.1ï¼Œè¨“ç·´æ¬¡æ•¸ 10 å›žåˆ
ppn.fit(X, y)

plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')
"""
æŠŠ æ¯ä¸€å€‹ epoch çŠ¯éŒ¯çš„æ¬¡æ•¸ (ppn.errors_) ç•«å‡ºä¾†ï¼
range(1, len(ppn.errors_) + 1)ï¼šä»£è¡¨ç¬¬ 1 å›žåˆã€ç¬¬ 2 å›žåˆã€...ä¸€ç›´åˆ°ç¬¬ 10 å›žåˆã€‚
marker='o'ï¼šæ¯å€‹é»žä¸Šç•«ä¸€å€‹å°åœˆåœˆï¼Œæ–¹ä¾¿çœ‹æ¸…æ¥šæ¯å›žåˆçš„éŒ¯èª¤æ•¸é‡"""
plt.xlabel('Epochs')
plt.ylabel('Number of updates')

# plt.savefig('images/02_07.png', dpi=300)
plt.show()



# ### A function for plotting decision regions




#ç•«æ±ºç­–é‚Šç•Œ
def plot_decision_regions(X, y, classifier, resolution=0.02):

    # setup marker generator and color map
    markers = ('s', 'x', 'o', '^', 'v')
    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')
    cmap = ListedColormap(colors[:len(np.unique(y))])#ListedColormapï¼šå»ºç«‹ä¸€å€‹ã€Œé¡è‰²å°æ‡‰è¡¨ã€ï¼Œä¸åŒé¡žåˆ¥ç”¨ä¸åŒé¡è‰²

    # plot the decision surface
    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    #(å®šç¾©ç•«åœ–ç¯„åœ)æ‰¾å‡ºç¬¬1ç¶­ã€ç¬¬2ç¶­ç‰¹å¾µçš„æœ€å°æœ€å¤§å€¼ï¼ŒåŠ æ¸›ä¸€é»ž marginï¼Œè®“åœ–ä¸æœƒå¤ªæ“ 
    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),
                           np.arange(x2_min, x2_max, resolution))
    #np.meshgridï¼šåœ¨æ•´å€‹ç¯„åœå…§é‹ªä¸€å±¤ç´°ç¶²æ ¼ã€‚
    #resolution æ˜¯ä½ æ¯éš”å¤šå°‘è·é›¢å–ä¸€å€‹é»žï¼ˆæ•¸å€¼è¶Šå°ï¼Œç¶²æ ¼è¶Šç´°ã€åœ–è¶Šå¹³æ»‘ï¼‰ã€‚
    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)
    Z = Z.reshape(xx1.shape)
    """æŠŠç¶²æ ¼å±•å¹³ (ravel) å¾Œé¤µé€² classifier é æ¸¬ã€‚
        Z å°±æ˜¯æ¯å€‹ç¶²æ ¼é»žè¢«åˆ†é¡žåˆ°å“ªä¸€é¡žã€‚
        ç„¶å¾Œ reshape å›žåŽŸæœ¬çš„ç¶²æ ¼å½¢ç‹€ï¼Œæ–¹ä¾¿ç•«ç­‰é«˜ç·šã€‚"""
    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)#contourfï¼šç•«å‡ºå„å€åŸŸå¡«è‰²ï¼ˆä¸åŒé¡žåˆ¥ä¸åŒé¡è‰²ï¼‰ï¼Œé€æ˜Žåº¦ 0.3ï¼Œç–Šåœ¨è³‡æ–™é»žä¸‹é¢ã€‚
    plt.xlim(xx1.min(), xx1.max())
    plt.ylim(xx2.min(), xx2.max())#è®“åº§æ¨™è»¸ç¯„åœå‰›å¥½åŒ…ä½æ‰€æœ‰ç¶²æ ¼ã€‚

    # plot class examples
    for idx, cl in enumerate(np.unique(y)):
        plt.scatter(x=X[y == cl, 0], 
                    y=X[y == cl, 1],
                    alpha=0.8, 
                    c=colors[idx],
                    marker=markers[idx], 
                    label=cl, 
                    edgecolor='black')
        """np.unique(y)ï¼šæ‰¾å‡ºæ‰€æœ‰é¡žåˆ¥ã€‚
            for idx, cl in enumerate(...)ï¼šå°æ¯å€‹é¡žåˆ¥ä¸€å€‹ä¸€å€‹ç•«ã€‚
            X[y == cl, 0] & X[y == cl, 1]ï¼šåªå–è©²é¡žåˆ¥çš„é»žã€‚
            alpha=0.8ï¼šé»žé»žçš„é€æ˜Žåº¦ã€‚
            c=colors[idx] & marker=markers[idx]ï¼šæ¯å€‹é¡žåˆ¥é…ä¸€ç¨®é¡è‰²å’Œå½¢ç‹€ã€‚"""

plot_decision_regions(X, y, classifier=ppn)
plt.xlabel('sepal length [cm]')
plt.ylabel('petal length [cm]')
plt.legend(loc='upper left')


# plt.savefig('images/02_08.png', dpi=300)
plt.show()



# # Adaptive linear neurons and the convergence of learning

# ...

# ## Minimizing cost functions with gradient descent





# ## Implementing an adaptive linear neuron in Python



class AdalineGD(object):
    """ADAptive LInear NEuron classifier.

    Parameters
    ------------
    eta : float
      Learning rate (between 0.0 and 1.0)
    n_iter : int
      Passes over the training dataset.
    random_state : int
      Random number generator seed for random weight
      initialization.


    Attributes
    -----------
    w_ : 1d-array
      Weights after fitting.
    cost_ : list
      Sum-of-squares cost function value in each epoch.

    """
    def __init__(self, eta=0.01, n_iter=50, random_state=1):
        self.eta = eta
        self.n_iter = n_iter
        self.random_state = random_state

    def fit(self, X, y):
        """ Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_examples, n_features]
          Training vectors, where n_examples is the number of examples and
          n_features is the number of features.
        y : array-like, shape = [n_examples]
          Target values.

        Returns
        -------
        self : object

        """
        rgen = np.random.RandomState(self.random_state)
        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])#åˆå§‹åŒ–æ¬Šé‡ï¼šself.w_ï¼ˆå°å°äº‚æ•¸ï¼‰
        self.cost_ = []

        for i in range(self.n_iter):
            net_input = self.net_input(X)
            # Please note that the "activation" method has no effect
            # in the code since it is simply an identity function. We
            # could write `output = self.net_input(X)` directly instead.
            # The purpose of the activation is more conceptual, i.e.,  
            # in the case of logistic regression (as we will see later), 
            # we could change it to
            # a sigmoid function to implement a logistic regression classifier.
            output = self.activation(net_input)
            errors = (y - output)
            self.w_[1:] += self.eta * X.T.dot(errors)
            self.w_[0] += self.eta * errors.sum()
            cost = (errors**2).sum() / 2.0
            self.cost_.append(cost)
            """è¨ˆç®— net inputï¼ˆåŠ æ¬Šç¸½å’Œï¼‰
                è¨ˆç®— outputï¼ˆé€™é‚Šæ˜¯ç·šæ€§å‡½æ•¸ï¼Œæ‰€ä»¥ output == net_inputï¼‰
                è¨ˆç®— errorï¼šerrors = (y - output)
                æ›´æ–°æ¬Šé‡ï¼š
                self.w_[1:] += self.eta * X.T.dot(errors)
                self.w_[0] += self.eta * errors.sum()
                è¨˜éŒ„ costï¼ˆSSE å¹³æ–¹èª¤å·®å’Œï¼‰costè¶Šå°è¶Šæº–"""
        return self

    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]#è¨ˆç®— åŠ æ¬Šç¸½å’Œï¼šð‘§=ð‘¤0+ð‘¤1ð‘¥1+ð‘¤2ð‘¥2+â€¦z=w0â€‹ +w1â€‹ x1 +w2 x2 +â€¦

    def activation(self, X):
        """Compute linear activation"""
        return X#é€™è£¡æ˜¯ ç·šæ€§å‡½æ•¸ï¼ˆç›´æŽ¥å‚³å›žä¾†ï¼‰ï¼Œ ï¼ˆç‚ºä»€éº¼è¦ç•™é€™å€‹ï¼Ÿ â†’ å› ç‚ºä»¥å¾Œå¦‚æžœæƒ³æ”¹æˆåˆ¥çš„ï¼Œä¾‹å¦‚ Logistic å‡½æ•¸ï¼Œåªè¦æ”¹é€™è£¡å°±å¥½äº†ã€‚ï¼‰

    def predict(self, X):
        """Return class label after unit step"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)
        #æ ¹æ“š outputï¼šå¦‚æžœ â‰¥ 0ï¼Œå°±é æ¸¬æˆ 1 å¦å‰‡é æ¸¬æˆ -1



fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))

ada1 = AdalineGD(n_iter=10, eta=0.01).fit(X, y)
ax[0].plot(range(1, len(ada1.cost_) + 1), np.log10(ada1.cost_), marker='o')
ax[0].set_xlabel('Epochs')
ax[0].set_ylabel('log(Sum-squared-error)')
ax[0].set_title('Adaline - Learning rate 0.01')

ada2 = AdalineGD(n_iter=10, eta=0.0001).fit(X, y)
ax[1].plot(range(1, len(ada2.cost_) + 1), ada2.cost_, marker='o')
ax[1].set_xlabel('Epochs')
ax[1].set_ylabel('Sum-squared-error')
ax[1].set_title('Adaline - Learning rate 0.0001')

# plt.savefig('images/02_11.png', dpi=300)
plt.show()

"""
ç°¡å–®ä¾†èªªï¼Œå­¸ç¿’çŽ‡å¤ªå° â†’ å­¸å¾—å¤ªæ…¢ï¼Œ
å­¸ç¿’çŽ‡å‰›å¥½ â†’ åˆå¿«åˆç©©å®šï¼Œ
å­¸ç¿’çŽ‡å¤ªå¤§ â†’ å¯èƒ½æœƒçˆ†ç‚¸éœ‡ç›ªã€‚(å¦‚åœ–ä¸­eta=0.01)"""






# ## Improving gradient descent through feature scaling







# standardize features æ¨™æº–åŒ–
X_std = np.copy(X)
X_std[:, 0] = (X[:, 0] - X[:, 0].mean()) / X[:, 0].std()
X_std[:, 1] = (X[:, 1] - X[:, 1].mean()) / X[:, 1].std()



#æ¨™æº–åŒ–ä¹‹å¾Œæ¯”è¼ƒä¸éœ€è¦æ“”å¿ƒetaå€¼éŽå¤§
ada_gd = AdalineGD(n_iter=15, eta=0.01)
ada_gd.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada_gd)
plt.title('Adaline - Gradient Descent')
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.legend(loc='upper left')
plt.tight_layout()
# plt.savefig('images/02_14_1.png', dpi=300)
plt.show()

plt.plot(range(1, len(ada_gd.cost_) + 1), ada_gd.cost_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Sum-squared-error')

plt.tight_layout()
# plt.savefig('images/02_14_2.png', dpi=300)
plt.show()



# ## Large scale machine learning and stochastic gradient descent



class AdalineSGD(object):
    """ADAptive LInear NEuron classifier.

    Parameters
    ------------
    eta : float
      Learning rate (between 0.0 and 1.0)
    n_iter : int
      Passes over the training dataset.
    shuffle : bool (default: True)
      Shuffles training data every epoch if True to prevent cycles.
    random_state : int
      Random number generator seed for random weight
      initialization.


    Attributes
    -----------
    w_ : 1d-array
      Weights after fitting.
    cost_ : list
      Sum-of-squares cost function value averaged over all
      training examples in each epoch.

        
    """
    """æ˜¯ Adaline çš„ä¸€ç¨®æ”¹è‰¯ç‰ˆï¼Œ
        æ”¹æˆä¸€æ¬¡åªç”¨ä¸€ç­†è³‡æ–™æ›´æ–°æ¬Šé‡ï¼ˆè€Œä¸æ˜¯åƒ AdalineGD é‚£æ¨£ï¼Œæ¯å€‹ epoch ç´¯ç©å®Œä¸€æ•´æ‰¹è³‡æ–™å†æ›´æ–°ï¼‰ã€‚
        æ”¯æ´ï¼š
        æ¯ epoch æ´—ç‰Œ (shuffle=True)ï¼Œé¿å…è³‡æ–™é †åºå½±éŸ¿å­¸ç¿’ã€‚
        partial_fit() â†’ å±€éƒ¨æ›´æ–°ï¼Œå¯ä»¥éš¨æ™‚ä¸Ÿæ–°è³‡æ–™ä¾†å¾®èª¿ã€‚
        é©åˆè³‡æ–™é‡å¤§"""
    
    def __init__(self, eta=0.01, n_iter=10, shuffle=True, random_state=None):
        self.eta = eta
        self.n_iter = n_iter
        self.w_initialized = False
        self.shuffle = shuffle
        self.random_state = random_state
        
    def fit(self, X, y):
        """ Fit training data.

        Parameters
        ----------
        X : {array-like}, shape = [n_examples, n_features]
          Training vectors, where n_examples is the number of examples and
          n_features is the number of features.
        y : array-like, shape = [n_examples]
          Target values.

        Returns
        -------
        self : object

        """
        self._initialize_weights(X.shape[1])
        self.cost_ = []
        for i in range(self.n_iter):
            if self.shuffle:
                X, y = self._shuffle(X, y)
            cost = []
            for xi, target in zip(X, y):
                cost.append(self._update_weights(xi, target))
            avg_cost = sum(cost) / len(y)
            self.cost_.append(avg_cost)
        return self

    def partial_fit(self, X, y):
        """Fit training data without reinitializing the weights"""
        if not self.w_initialized:
            self._initialize_weights(X.shape[1])
        if y.ravel().shape[0] > 1:
            for xi, target in zip(X, y):
                self._update_weights(xi, target)
        else:
            self._update_weights(X, y)
        return self

    def _shuffle(self, X, y):
        """Shuffle training data"""
        r = self.rgen.permutation(len(y))
        return X[r], y[r]
    
    def _initialize_weights(self, m):
        """Initialize weights to small random numbers"""
        self.rgen = np.random.RandomState(self.random_state)
        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)
        self.w_initialized = True
        
    def _update_weights(self, xi, target):
        """Apply Adaline learning rule to update the weights"""
        output = self.activation(self.net_input(xi))
        error = (target - output)
        self.w_[1:] += self.eta * xi.dot(error)
        self.w_[0] += self.eta * error
        cost = 0.5 * error**2
        return cost
    
    def net_input(self, X):
        """Calculate net input"""
        return np.dot(X, self.w_[1:]) + self.w_[0]

    def activation(self, X):
        """Compute linear activation"""
        return X

    def predict(self, X):
        """Return class label after unit step"""
        return np.where(self.activation(self.net_input(X)) >= 0.0, 1, -1)




ada_sgd = AdalineSGD(n_iter=15, eta=0.01, random_state=1)
ada_sgd.fit(X_std, y)

plot_decision_regions(X_std, y, classifier=ada_sgd)
plt.title('Adaline - Stochastic Gradient Descent')
plt.xlabel('sepal length [standardized]')
plt.ylabel('petal length [standardized]')
plt.legend(loc='upper left')

plt.tight_layout()
# plt.savefig('images/02_15_1.png', dpi=300)
plt.show()

plt.plot(range(1, len(ada_sgd.cost_) + 1), ada_sgd.cost_, marker='o')
plt.xlabel('Epochs')
plt.ylabel('Average Cost')

plt.tight_layout()
# plt.savefig('images/02_15_2.png', dpi=300)
plt.show()




ada_sgd.partial_fit(X_std[0, :], y[0])



# # Summary

# ...

# --- 
# 
# Readers may ignore the following cell




