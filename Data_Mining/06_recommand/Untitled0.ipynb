{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP/7VCggQaBSYxo4zQcv3ks"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, Flatten, Dense\n","\n","# Sample corpus\n","sentences = [\n","    'I love machine learning',\n","    'Deep learning is a branch of machine learning',\n","    'Word embeddings are a type of word representation',\n","    'Machine learning is awesome',\n","    'Word2Vec is a popular word embedding method',\n","]\n","\n","# Tokenizing the sentences\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","\n","# Converting sentences to sequences of integers\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded_sequences = pad_sequences(sequences, padding='post')\n","\n","# Parameters\n","vocab_size = len(word_index) + 1  # Adding 1 because of reserved 0 index\n","embedding_dim = 8  # Dimension of the embedding space\n","input_length = padded_sequences.shape[1]  # Length of input sequences\n","\n","# Model\n","model = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length),\n","    Flatten(),\n","    Dense(1, activation='sigmoid')\n","])\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy')\n","\n","# Train the model (Note: In this demo, we won't actually use labels)\n","model.fit(padded_sequences, np.zeros((len(sentences), 1)), epochs=10, verbose=0)\n","\n","# Extract the embeddings\n","embeddings = model.layers[0].get_weights()[0]\n","\n","# Displaying the embeddings\n","for word, i in word_index.items():\n","    print(f'Word: {word} - Embedding: {embeddings[i]}')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQsQPnatwvSp","executionInfo":{"status":"ok","timestamp":1725331681216,"user_tz":-480,"elapsed":2293,"user":{"displayName":"joseph chen","userId":"10452885981021749798"}},"outputId":"c409e7e3-f30c-490f-9969-487ad6a92430"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Word: learning - Embedding: [-0.01715189 -0.00602535 -0.05668322  0.04518725 -0.04940545 -0.00952972\n","  0.00926516  0.02820825]\n","Word: machine - Embedding: [ 0.01934711 -0.01436297  0.01959456 -0.00916991  0.00155818  0.05679836\n","  0.01975585 -0.01941023]\n","Word: is - Embedding: [-0.03567606  0.02820829  0.01609422  0.00899245 -0.03972582  0.04710207\n"," -0.01695937  0.04641515]\n","Word: a - Embedding: [-0.01446102 -0.00046867 -0.05311938  0.0120619  -0.01417905 -0.01092705\n","  0.0262667  -0.02308758]\n","Word: word - Embedding: [-0.04550141  0.00797239 -0.0342592  -0.04156759 -0.0074445  -0.00364592\n","  0.03261053 -0.047578  ]\n","Word: of - Embedding: [-0.02805678 -0.01814794  0.02440405 -0.02318072 -0.03416475 -0.02622538\n"," -0.03294413 -0.06069958]\n","Word: i - Embedding: [-0.00944831 -0.04214373 -0.04383896  0.01744683  0.02208924 -0.02028611\n","  0.04576056 -0.03880486]\n","Word: love - Embedding: [ 0.01807635  0.05958747 -0.02930271  0.03012785 -0.02237262  0.01292581\n"," -0.03431427  0.00385551]\n","Word: deep - Embedding: [-0.02085697  0.0246702   0.02376414  0.02478022  0.01866616  0.05500178\n"," -0.02103087  0.00854919]\n","Word: branch - Embedding: [ 0.02324262 -0.03972678 -0.04552007 -0.00713868 -0.03744961  0.0443519\n"," -0.02550785 -0.00699322]\n","Word: embeddings - Embedding: [-0.03844088  0.02188215 -0.00999397  0.01565422 -0.04857675  0.03039973\n","  0.02663907 -0.02136498]\n","Word: are - Embedding: [-0.01988664  0.04789755  0.02475621 -0.01579259  0.00717548  0.02472742\n","  0.0065853   0.03692506]\n","Word: type - Embedding: [ 0.0057504   0.01373246 -0.05129544 -0.03195737  0.03238205  0.05260799\n"," -0.00122295 -0.0175486 ]\n","Word: representation - Embedding: [-0.0152733  -0.03702608 -0.02635475  0.03347937 -0.03986862 -0.0293399\n"," -0.05858186  0.00464889]\n","Word: awesome - Embedding: [ 0.02879211 -0.03371166 -0.0477982   0.03514996 -0.05545616 -0.03158282\n","  0.0276557   0.04559143]\n","Word: word2vec - Embedding: [ 0.00570018 -0.0220053   0.02025053 -0.03139729 -0.00037923 -0.0027565\n"," -0.02247335 -0.05442955]\n","Word: popular - Embedding: [ 0.03722059 -0.00147514 -0.05308576  0.05218986 -0.04326463 -0.03232772\n","  0.03216872 -0.02188306]\n","Word: embedding - Embedding: [0.0464841  0.02872    0.01615903 0.05953332 0.05165892 0.01439953\n"," 0.01552764 0.00585501]\n","Word: method - Embedding: [-0.00222978 -0.03978232  0.01385954 -0.03181996 -0.05412657  0.0148745\n"," -0.02111451 -0.03276187]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics.pairwise import cosine_similarity"],"metadata":{"id":"puebeZirxJRo","executionInfo":{"status":"ok","timestamp":1725331841591,"user_tz":-480,"elapsed":1387,"user":{"displayName":"joseph chen","userId":"10452885981021749798"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["similarity12 = cosine_similarity([embedding1], [embedding2])"],"metadata":{"id":"FIR1CKhNyscD"},"execution_count":null,"outputs":[]}]}